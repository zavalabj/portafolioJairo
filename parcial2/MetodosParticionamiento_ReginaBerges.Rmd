---
title: "Métodos de particionamiento"
author: "Regina Berges"
date: "30 de marzo de 2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

El código a continuación genera los datos con los que se trabajará este proyecto. 

```{r cars}
library(mvtnorm)
# Generamos los datos
n1 <- n2 <- n3 <- 100
d <- 3    # día de nacimiento
m <- 2    # mes de nacimiento
a <- 1999 # año de nacimiento

d1 <- round(d/30, 2)
d2 <- round(m/12, 2)
d3<-round(a/2020, 2)

# Matrices de covarianza
s1 <- matrix(c(1, d1, d1, 1), ncol = 2)
s2 <- matrix(c(1.5, d2, d2, 1.5), ncol = 2)
s3 <- matrix(c(2, d3, d3, 2), ncol = 2)

# Vector de medias
m1 <- c(m,m)
m2 <- m1+m
m3 <- m2+m

set.seed(a)
x1 <-rmvnorm(n1, m1, s1)
x2 <-rmvnorm(n2, m2, s2)
x3 <-rmvnorm(n3, m3, s3)

x <- c(x1[,1], x2[,1], x3[,1])
y <- c(x1[,2], x2[,2], x3[,2])

# Datos para trabajar
dataset <- data.frame(cbind(x, y))

```

## Análisis exploratorio

```{r pressure, echo=FALSE}
summary(dataset)
```


```{r pressure, echo=FALSE, warning=FALSE, tidy=TRUE}
library(ggplot2)

# Escalar datos 
data <-data.frame(scale(dataset))

ggplot()+ geom_point(aes(x = x, y = y), data = dataset, alpha = 0.5) + ggtitle('Conjunto de datos')
```

La primera impresión que se obtiene al observar esta gráfica es que realmente no hay una clara separación de las observaciones, lo que significa que será complicado realizar agrupaciones de las mismas.



## 1. Métodos de particionamiento

### K-Medias

Para determinar la cantidad óptima de centroides k a utilizar se aplicará el *Método del Codo*. Básicamente, esta técnica utiliza los valores de la inercia obtenidos tras aplicar el K-medias a diferente número de agrupaciones (desde $1$ a $n$ grupos), siendo la inercia la suma de las distancias al cuadrado de cada objeto del Cluster a su centroide.

```{r pressure, echo=FALSE}
library(factoextra)
fviz_nbclust(data,kmeans, method = "wss",k.max=10,
             diss = get_dist(data, method = "euclidean"), nstart = 50)+
labs(title= "Número óptimo de agrupaciones") + 
  xlab("k ") +
  ylab("Suma de cuadrados internos")
```

Para seleccionar el valor óptimo de k, se debe seleccionar aquel punto en donde ya no se dejan de producir variaciones importantes del valor de WSS al aumentar k. En este caso, esto se produce a partir de $k≥3$, por lo que se evaluarán los resultados del agrupamiento con ese valor, a fin de observar el comportamiento del modelo.Pero antes de realizar la comprobación del modelo, y a fin de demostrar que el valor de $k$ con el que se trabajará es el número óptimo de cluster, se usará el *método silhouette*, para que nos arroje automáticamente el valor ideal de $k$.

```{r pressure, echo=FALSE}
set.seed(a)
fviz_nbclust(data, kmeans, method = "silhouette",k.max = 10,
             diss = get_dist(data, method = "euclidean"), nstart = 50)+
  labs(title= "Número óptimo de agrupaciones") + 
  xlab("k ") +
  ylab("Ancho medio entre grupos")

```

Se hará una tercera y última comprobación utilizando el método de *estadísticas gap*.

```{r pressure, echo=FALSE,warning=FALSE, tidy=TRUE}
set.seed(a)
fviz_nbclust(data, kmeans, method = "gap_stat", k.max = 10, 
             diss = get_dist(my_data, method = "euclidean"), nstart = 50)+
  labs(title= "Número óptimo de cluster") + 
  xlab("k ") +
  ylab("Estadísticas Gap")
```

Se puede observar que este método, al igual que los otros dos empleados, nos indica que $k=3$ es el número óptimo de clusters con el que se debe trabajar. El siguiente paso es utilizar la técnica de k-medias con dicho valor de k.

```{r pressure, echo=FALSE}
set.seed(a)
kmedias <- kmeans(data, 3, iter.max = 1000, nstart = 50)
data$cluster <- kmedias$cluster
ggplot() + geom_point(aes(x = x, y = y, color = cluster), data = data, size = 2) +
  scale_colour_gradientn(colours=rainbow(3)) +
  geom_point(aes(x = kmedias$centers[, 1], y = kmedias$centers[, 2]), color = 'black', size = 3) + 
  ggtitle('Clasificación óptima') + 
  xlab('X') + ylab('Y')
```

### K-Medoides

```{r pressure, echo=FALSE,warning=FALSE, tidy=TRUE}
library("cluster")
kmedoides <- pam(data, 3)
fviz_cluster(kmedoides, data = data)

```

### Clara

```{r pressure, echo=FALSE}
clarax <- clara(data, 3)
fviz_cluster(clarax, data = data)
```

### Cluster jerárquico

```{r pressure, echo=FALSE}
res <- hcut(data, k = 3)
fviz_dend(res, rect = TRUE, cex = 0.5,
          k_colors = c("red","blue", "green")) 
```


## 2. Método de evaluación

```{r pressure, echo=FALSE, warning=FALSE, tidy=TRUE}
library("clValid")
validclus  <- clValid(data, nClust = 2:6, 
                clMethods = c("hierarchical","kmeans","pam","clara"),
                validation = "internal")
summary(validclus)
plot(validclus)
```


