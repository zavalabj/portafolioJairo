---
title: "Corrección Examen Parcial 1"
author: "ByteMiners"
date: "09/03/2020"
output:
  pdf_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Análisis exploratorio de datos
Las librerías usadas en el examen fueron las siguientes:
```{r, echo=TRUE,message=FALSE,warning=FALSE}
library(ggplot2)
library(cowplot)
library(dplyr)
library(grid)
library(FactoMineR)
library(factoextra)
library(corrplot)
library(ggpubr)
library(scatterplot3d)
library(reshape2)
library(knitr)
library(MVN)
library(biotools)
library(klaR)
library(janitor)
library(psych)
library(gridExtra)
library(GGally)
library(funModeling)
```
# 1. Exploración y visualización de variables

## 1.1 Explorar la base de datos, realizar los cambios necesarios y convenientes para poder trabajar con ella.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE} 
df <- read.csv("Ejercicio 1.csv", sep = ",")
df
```

Como podemos observar esta base de datos cuenta con 10 variables o características, las cuales son: Peso, Estatura, Edad, Complexión, Papas separados, Cantidad hermanos, Trabaja, Horas trabajadas por día, Horas trabajadas por mes y salario mensual.

Además, es posible notar que se conforma de 50 observaciones, sin embargo es necesario reordenar los datos, ya que cada observación debe estar contenida en una fila y no en una columna (como se nos fue otorgada la información inicialmente).

Lo primero que realizamos fue la trasposición de las columnas del dataframe para que cada observación fuera vista en una fila y no por columnas como inicialmente estaba acomodada la información.Después es necesario colocar ceros en los campos en donde hay NA para poder realizar un análisis más acertado de los datos.

Otra cosa que debemos arreglar son las observaciones en donde el valor de la característica "Trabaja" es "NO", debido a que contienen variables relacionadas con la anteriormente mencionada (las cuales son: "Horas_trab_dia", "Horas_trab_mes" y "Salario_mes") y que se encuentran vacías. Esto último igual puede afectar nuestro análisis, es por eso que decidimos colocar ceros en aquellos campos en donde existen NA.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE} 

#Trasponer las columnas del df para que las características de cada observacion estén ordenadas por columnas 
df_transposed <- as.data.frame(t(as.matrix(df)))

# Colocar nombre a las columnas y eliminar la primera fila 
names(df_transposed) <- c("Peso","Estatura","Edad","Complexion","Papas_separados","Cantidad_hermanos",
                                  "Trabaja", "Horas_trab_dia", "Horas_trab_mes", "Salario_mes")

datos <- df_transposed[-c(1),] 

# Reemplazar valores NA con 0 solo en las columnas seleccionadas
vars_to_replace=c("Horas_trab_dia", "Horas_trab_mes", "Salario_mes")

datos = datos %>% mutate_at(.vars=vars_to_replace, .funs = funs(ifelse(is.na(.), 0, .)))

datos
```

Para continuar con el análisis exploratorio, es momento de distinguir el tipo de dato correspondiente para cada variable que conforma esta base de datos. Esto con el fin de poder determinar cómo explorar la base de datos y si es necesario modificar algún tipo de datos para realizar tablas o gráficas relevantes.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE} 
# Analizar la base de datos
df_status(datos)
```

Existen 7 variables de tipo "factor" el cual es usado para variables categóricas o nominales, lo curioso es que de esas 7 variables, solo 3 (Complexión, Papas separados y Trabaja) son categóricas y las restantes (Peso, Estatura, Edad y Cantidad hermanos) deberían ser numéricas.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE} 
# Convertir en valores númericos aquellos almacenados en las columnas seleccionadas
vars_to_replace=c("Peso", "Estatura", "Edad", "Cantidad_hermanos")

datos[vars_to_replace] <- lapply(datos[vars_to_replace], function(x) as.numeric(as.character(x)))

df_status(datos)
```

Otra corrección que se debe realizar es hacer función piso a los valores de Edad, ya que estos fueron ingresados como decimales y eso es erróneo, deben ser considerados como discretos.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
datos['Edad'] <- floor(datos['Edad'])

```

## 1.2  Expliquen por medio de tablas esta información. En la tabla debe aparecer simultáneamente la frecuencia y el porcentaje

Es importante recalcar que continuamos realizando análisis exploratorio de la base de datos y respetando las instrucciones de cada subinciso, es por esto que ahora elaboraremos tablas que contengan información sobre la frecuencia y porcentaje de aquellas variables que lo requieren, como lo son: Complexión, Papás separados, Cantidad hermanos, Trabaja y Horas diarias trabajadas. 

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}

#Tabla de frecuencias de complexion
frecuency_as_df<-as.data.frame(tabyl(datos$Complexion, sort = TRUE))

names(frecuency_as_df) <- c("Complexion","Frecuencia","Porcentaje")
final_constitution_frecuency<-frecuency_as_df[-c(1),]

final_constitution_frecuency
```

A partir de esta tabla podemos inferir que el 56% de las personas que se encuentran registradas en la base de datos tienen un cuerpo mesomorfo, es decir, su complexión es intermedia (ni muy delgada ni muy gruesa) y se caracterizan por tener un metabolismo regular.

Por otro lado se observa que el 36% de las observaciones tienen un cuerpo endomorfo, lo que significa que desgraciadamente sufren de sufre peso.

Y aquellos con complexión ectomorfa (muy delgada) solo cubren el 8% de nuestras observaciones.

En conclusión se puede decir que más de la mitad de las personas se encuentran saludables y con una buena complexión, sin embargo el 44% restante debería acudir con un médico y estar al pendiente de su salud.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
#Tabla de frecuencias de papás_separados
frecuency_as_df<-as.data.frame(tabyl(datos$Papas_separados, sort = TRUE))

names(frecuency_as_df) <- c("Papás_separados","Frecuencia","Porcentaje")
final_separated_parents_frecuency<-frecuency_as_df[-c(2),]

final_separated_parents_frecuency
```

Sabemos que solo el 20% de los individuos registrados en la base de datos tiene padres separados, esto puede ser por diversas razones, las cuales desconocemos. Esta información puede ser útil para evaluar si el desempeño de una persona o nivel de carga de trabajo se ve influenciado por la relación de sus progenitores.


```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
#Tabla de frecuencias de cantidad_hermanos
frecuency_as_df<-as.data.frame(tabyl(datos$Cantidad_hermanos, sort = TRUE))

names(frecuency_as_df) <- c("Cantidad_hermanos","Frecuencia","Porcentaje")
final_number_of_brothers_frecuency<-frecuency_as_df[-c(7),]

final_number_of_brothers_frecuency
```

Con esta tabla se puede inferir que en esta base de datos predominan las observaciones con menos de 3 hermanos, mientras que aquellas observaciones con más de 3 hermanos son menos frecuentes.
El número de hermanos es un dato relevante para estudiar si el nivel socioeconómico de una persona se ve afectado por la cantidad de hijos que deciden criar sus padres.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
#Tabla de frecuencias de los que trabajan
frecuency_as_df<-as.data.frame(tabyl(datos$Trabaja, sort = TRUE))

names(frecuency_as_df) <- c("Trabaja","Frecuencia","Porcentaje")
final_iswork_frecuency<-frecuency_as_df[-c(3),]

final_iswork_frecuency
```

De igual forma es importante destacar que el 72% de las personas registradas en la base de datos sí trabaja, mientras que el 28% restante no lo hace, esto puede ser debido a que esos individuos son menores de edad o por factores que no son posibles determinar con la información otorgada por la base de datos.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
#Filtro de las observaciones que SI trabajan

trabajan <- filter(datos, Trabaja == "SI")

#Tabla de frecuencias de horas trabajadas x dia

frecuency_as_df<-as.data.frame(tabyl(trabajan$Horas_trab_dia, sort = TRUE))

names(frecuency_as_df) <- c("Horas_trabajo x dia","Frecuencia","Porcentaje")
final_worktime_per_day_frecuency<-frecuency_as_df[-c(8),]

final_worktime_per_day_frecuency

```

Con esta tabla podemos observar que las personas registradas en la base de datos que sí trabajan, lo hacen en promedio entre 1 a 7 horas al día y que el 25% de ellas prefiere laborar medio día (4 horas).


Continuando con el análisis, es momento de enfocarnos en las otras variables de la base de datos (Peso, Estatura, Edad, Horas trabajadas al mes y Salario mensual), aquellas que no colocamos en tablas pues la frecuencia de sus datos es de 1 y sus tablas de frecuencia y porcentajes resultaban poco agradables visualmente.

Lo que haremos con ellas es visualizar un resumen de las mismas.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
summary(datos$Peso)
sd(datos$Peso)

```

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
summary(datos$Estatura)
sd(datos$Estatura)

```

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
summary(datos$Edad)
sd(datos$Edad)

```

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
summary(trabajan$Horas_trab_mes)
sd(trabajan$Horas_trab_mes)

```

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
summary(trabajan$Salario_mes)
sd(trabajan$Salario_mes)

```

## 1.3 Hagan un gráfico donde pueda explicar las variables dependientes.


```{r, echo=TRUE,warning=FALSE,tidy=TRUE, message=FALSE}
numericas <- datos %>% dplyr::select(Peso, Estatura, Edad,Horas_trab_dia, 
                                     Horas_trab_mes, Salario_mes)
ggpairs(numericas, title = "Variables númericas dependientes")
```


```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
plot(datos$Complexion)
```
```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
plot(datos$Trabaja)
```


```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
plot(datos$Papas_separados)
```



## 1.4 Construyan diferentes gráficos donde puedan mostrar patrones y detalles de la base de datos con respecto a una nueva variable que podemos denominar: “categoría”


```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
# Número de personas por categoría
cantidad_personas <- c(sum(with(datos, Edad < 18.0)), 
                       sum(with(datos, Edad >=18.0 & Edad<=25.0)),
                       sum(with(datos, Edad > 25.0)))

dist_personas <- data.frame(Categoria=c("Junior", "Señor", "Mayor"), Cantidad=cantidad_personas)
ggplot(dist_personas, aes(x=Categoria, y=Cantidad)) + geom_bar(stat = "identity") + coord_flip() + labs(title="Distribución de personas por categoría") +
  geom_text(aes(y=Cantidad, label=cantidad_personas), vjust=1.5, color="white", size=3.5, hjust="right")
```

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
# Salario mensual de una persona dependiendo su categoría
junior <- filter(datos, Edad <18.0)
senior <- filter(datos, Edad>=18.0 & Edad <=25.0)
mayor <- filter(datos, Edad > 25.0)


```


```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
# Número de personas con padres separados por categoría
cantidad_personas <- c(sum(with(junior, Papas_separados == "SI")), 
                       sum(with(senior, Papas_separados == "SI")),
                       sum(with(mayor, Papas_separados == "SI")))

dist_personas <- data.frame(Categoria=c("Junior", "Señor", "Mayor"), Cantidad=cantidad_personas)
ggplot(dist_personas, aes(x=Categoria, y=Cantidad)) + geom_bar(stat = "identity") + coord_flip() + labs(title="Número de personas con padres separados por categoría")+
  geom_text(aes(y=Cantidad, label=cantidad_personas), vjust=1.5, color="white", size=3.5, hjust="right")

```

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
# Personas que trabajan dependiendo su categoría

```


# 2. Metodo de reducción de dimensión (PCA)

## 2.1 PCA base de datos PCA1
Para esta base de datos decidimos aplicar el modelo PCA
Como primer paso sacamos el porcentaje de varianza y la graficamos.
Una vez ya obtenido este punto proseguimos a graficar la correlación.
```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
df2<-read.csv("Ejercicio 2.1.csv", sep = ",")
res.pca <- PCA(df2[,-1], graph = F)

eig.val <- get_eigenvalue(res.pca)
eig.val
fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 90))

var <- get_pca_var(res.pca)

corrplot(var$cos2, is.corr=FALSE)
```
Proseguimos a graficar las variables y de igual manera graficamos las observaciones.
```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
fviz_pca_var(res.pca, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE # Avoid text overlapping
)

fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")
)

fviz_pca_ind(res.pca,
             geom.ind = "point", # show points only (nbut not "text")
             col.ind =df2$tipo, # color by groups
             palette = c("#00AFBB", "#E7B800", "#FC4E07"),
             addEllipses = TRUE, # Concentration ellipses
             legend.title = "Groups")
```
El modelo logró agrupar correctamente las observaciones de la base de datos.

## 2.2 PCA base de datos PCA2
Aplicamos el modelo PCA para obtener el numero óptimo de componentes principales, debido que el conjunto de datos contaba más de 200 variables.
Graficando la varianza acumulada podremos obtener lo siguiente:
```{r, echo=TRUE,tidy=TRUE}
url<-"http://web.stanford.edu/~hastie/ElemStatLearn//datasets/zip.digits/train.2"
data<-read.csv(url)

data.pca = scale(data)
data.pca <- prcomp(data, center = F, scale=T)
prop_varianza <- data.pca$sdev^2 / sum(data.pca$sdev^2)
prop_varianza_acum <- cumsum(prop_varianza)

ggplot(data = data.frame(prop_varianza, pc = 1:256),
       aes(x = pc, y = prop_varianza)) +
  xlim(0, 50) +
  geom_col(width = 0.3) +
  scale_y_continuous(limits = c(0,.5)) +
  theme_bw() +
  labs(x = "Componente principal",
       y = "Prop. de varianza explicada")

ggplot(data = data.frame(prop_varianza_acum, pc = 1:256),
       aes(x = pc, y = prop_varianza_acum, group = 1)) +
  geom_point() +
  geom_line() +
  geom_label(aes(label = round(prop_varianza_acum,2))) +
  theme_bw() +
  labs(x = "Componente principal",
       y = "Prop. varianza explicada acumulada")
```
Una vez viendo la representación grafica podemos concluir que un numero óptimo de dimenciones para esta base de datos sería aproximadamente 10.

# 3. Metodo de reducción de dimensión (LDA)

## 3.1 Exploren los datos y encuentren gráficamente sus tendencias y sus posibles correlaciones en gráficos 2D y 3D.
Graficamos los datos y sus correlaciones mediante un modelo 2D y 3D.En la grafica 2D podemos apreciar que las variables tienen una distribución normal y la relación entre el largo y el ancho generan agrupaciones las cuales están bien definidos.
```{r, echo=TRUE,tidy=TRUE}
datos<-read.csv("Ejercicio 3.csv", sep = ",")
p1 <- ggplot(data = datos, aes(x = largo, fill = tipo)) +
  geom_histogram(position = "identity", alpha = 0.5)
p2 <- ggplot(data = datos, aes(x = ancho, fill = tipo)) +
  geom_histogram(position = "identity", alpha = 0.5)
p3 <- ggplot(data = datos, aes(x = alto, fill = tipo)) +
  geom_histogram(position = "identity", alpha = 0.5)
ggarrange(p1, p2, p3, nrow = 3, common.legend = TRUE)
pairs(x = datos[, c("largo","ancho","alto")],
      col = c("blue", "green","red")[datos$tipo], pch = 19,main="Grafica Correlacion 2D")

scatterplot3d(datos$largo, datos$ancho, datos$alto,
              color = c("firebrick", "green","blue")[datos$tipo], pch = 19,
              grid = TRUE, xlab = "largo", ylab = "ancho",
              zlab = "alto", angle = 65, cex.axis = 0.6,main = "Visualizacion 3D")
legend("topleft",
       bty = "n", cex = 0.8,
       title = "tipo",
       c("rectangular", "cuadrada", "circular"), fill = c("firebrick", "green","blue"))
```
## 3.2 Realicen la verificación de supuestos.
Aplicamos las técnicas vistas en clase para evaluar la normalidad multivariable y para identificar outlayers que influyen en el comportamiento los datos, mediante los test Mardia, Henze-Zirkler y Royston
```{r, echo=TRUE,tidy=TRUE}
datos_tidy <- melt(datos, value.name = "valor")
kable(datos_tidy %>% group_by(tipo, variable) %>% summarise(p_value_Shapiro.test =
                                                              shapiro.test(valor)$p.value))
outliers <- mvn(data = datos[,-1], mvnTest = "hz", multivariateOutlierMethod = "quan")
royston_test <- mvn(data = datos[,-1], mvnTest = "royston", multivariatePlot = "qq")
royston_test$multivariateNormality
hz_test <- mvn(data = datos[,-1], mvnTest = "hz")
hz_test$multivariateNormality
boxM(data = datos[,2:4], grouping = datos[,1])
```
## 3.3 Apliquen el método LDA o QDA según consideren conveniente paso a paso.
Para esta base de datos decidimos aplicar el modelo LDA, ya que los datos como vimos anteriormente tienen una distribución normal.
```{r, echo=TRUE,tidy=TRUE}
modelo_lda <- lda(formula = tipo ~ largo + ancho + alto,
                  data = datos)
```
## 3.4 Grafiquen como quedaría la clasificación y calculen su error para nuevos datos.
Al aplicar el modelo de predicción para una nueva observación y al evaluarla nos dio un error del 0% lo cual indica que el modelo funciona correctamente.
```{r, echo=TRUE,tidy=TRUE}
nuevas_observaciones <- data.frame(largo = 30, ancho = 15,
                                   alto = 20)
predict(object = modelo_lda, newdata = nuevas_observaciones)
predicciones <- predict(object = modelo_lda, newdata = datos[, -1],
                        method = "predictive")
table(datos$tipo, predicciones$class,
      dnn = c("Clase real", "Clase predicha"))
trainig_error <- mean(datos$especie != predicciones$class) * 100
paste("trainig_error=", trainig_error, "%")
with(datos, {
  s3d <- scatterplot3d(largo, ancho, alto,
                       color = c("firebrick", "green","blue")[datos$tipo],
                       pch = 19, grid = TRUE, xlab = "largo", ylab = "ancho",
                       zlab = "altura", angle = 65, cex.axis = 0.6,main = "Clasificacion de los Nuevos Datos")
  
  s3d.coords <- s3d$xyz.convert(largo, ancho, alto)
  # convierte coordenadas 3D en proyecciones 2D
  
  legend("topleft", 
         bty = "n", cex = 0.8,
         title = "Tipo",
         c("largo", "ancho","alto"), fill = c("firebrick", "green","blue"))
})
```
Empleando las mismas observaciones con las que se ha generado el modelo discriminante (trainig data), la precisión de clasificación es del 100%.