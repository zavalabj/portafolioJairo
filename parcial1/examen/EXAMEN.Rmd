---
title: "Corrección Examen Parcial 1"
author: "ByteMiners"
date: "09/03/2020"
output:
  pdf_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Análisis exploratorio de datos
Las librerías usadas en el examen fueron las siguientes:
```{r, echo=TRUE,message=FALSE,warning=FALSE}
library(ggplot2)
library(cowplot)
library(dplyr)
library(grid)
library(FactoMineR)
library(factoextra)
library(corrplot)
library(ggpubr)
library(scatterplot3d)
library(reshape2)
library(knitr)
library(MVN)
library(biotools)
library(klaR)
library(janitor)
library(psych)
library(gridExtra)
library(GGally)
library(funModeling)
```
# 1. Exploración y visualización de variables

## 1.1 Explorar la base de datos, realizar los cambios necesarios y convenientes para poder trabajar con ella.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE} 
df <- read.csv("Ejercicio 1.1.csv", sep = ",")
df
```

Como podemos observar esta base de datos cuenta con 10 variables o características, las cuales son: Peso, Estatura, Edad, Complexión, Papas separados, Cantidad hermanos, Trabaja, Horas trabajadas por día, Horas trabajadas por mes y salario mensual.

Además, es posible notar que se conforma de 50 observaciones, sin embargo es necesario reordenar los datos, ya que cada observación debe estar contenida en una fila y no en una columna (como se nos fue otorgada la información inicialmente).

Lo primero que realizamos fue la trasposición de las columnas del dataframe para que cada observación fuera vista en una fila y no por columnas como inicialmente estaba acomodada la información.

Otra cosa que debemos arreglar son las observaciones en donde el valor de la característica "Trabaja" es "NO", debido a que contienen variables relacionadas con la anteriormente mencionada (las cuales son: "Horas_trab_dia", "Horas_trab_mes" y "Salario_mes") y que se encuentran vacías. Esto último puede afectar nuestro análisis, es por eso que decidimos colocar ceros en aquellos campos en donde existen NA.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE} 

#Trasponer las columnas del df para que las características de cada observacion estén ordenadas por columnas 
df_transposed <- as.data.frame(t(as.matrix(df)))

# Colocar nombre a las columnas y eliminar la primera fila 
names(df_transposed) <- c("Peso","Estatura","Edad","Complexion","Papas_separados","Cantidad_hermanos",
                                  "Trabaja", "Horas_trab_dia", "Horas_trab_mes", "Salario_mes")

datos <- df_transposed[-c(1),] 

# Reemplazar valores NA con 0 solo en las columnas seleccionadas
vars_to_replace=c("Horas_trab_dia", "Horas_trab_mes", "Salario_mes")

datos = datos %>% mutate_at(.vars=vars_to_replace, .funs = funs(ifelse(is.na(.), 0, .)))

datos
```

Para continuar con el análisis exploratorio, es momento de distinguir el tipo de dato correspondiente para cada variable que conforma esta base de datos. Esto con el fin de poder determinar cómo explorar la base de datos y si es necesario modificar algún tipo de dato para realizar tablas o gráficas relevantes.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE} 
# Analizar la base de datos
df_status(datos)
```

Existen 7 variables de tipo "factor" el cual es usado para variables categóricas o nominales, lo curioso es que de esas 7 variables, solo 3 (Complexión, Papas separados y Trabaja) son categóricas y las restantes (Peso, Estatura, Edad y Cantidad hermanos) deberían ser numéricas.

Para trabajar con las variables Horas_trab_dia, Horas_trab_mes y Salario_mes también se debe realizar un ajuste a su tipo de dato, porque como se pudo observar, al momento de cambiar los NA por ceros los valores almacenados en esos cambios fueron alterados.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE} 
# Convertir en valores númericos aquellos almacenados en las columnas seleccionadas
vars_to_replace=c("Peso", "Estatura", "Edad", "Cantidad_hermanos")

datos[vars_to_replace] <- lapply(datos[vars_to_replace], function(x) as.numeric(as.character(x)))

# Cargamos csv con los valores previamente almacenados en los campos Horas_trab_dia, Horas_trab_mes y Salario_mes
df2 <- read.csv("Ejercicio 1.csv", sep = ",")

#Trasponer las columnas del df para que las características de cada observacion estén ordenadas por columnas 
df2 <- as.data.frame(t(as.matrix(df2)))

# Colocar nombre a las columnas y eliminar la primera fila 
names(df2) <- c("Peso","Estatura","Edad","Complexion","Papas_separados","Cantidad_hermanos",
                                  "Trabaja", "Horas_trab_dia", "Horas_trab_mes", "Salario_mes")

df2 <- df2[-c(1),] 

datos$Horas_trab_dia <- as.numeric(as.numeric_version(df2$Horas_trab_dia))
datos$Horas_trab_mes <- as.numeric(as.numeric_version(df2$Horas_trab_mes))
datos$Salario_mes <- as.numeric(as.numeric_version(df2$Salario_mes))

status(datos)
```

Otra corrección que se debe realizar es hacer función piso a los valores de Edad, ya que estos fueron ingresados como decimales y eso es erróneo porque deben ser considerados como discretos.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
datos['Edad'] <- floor(datos['Edad'])

```

## 1.2  Expliquen por medio de tablas esta información. En la tabla debe aparecer simultáneamente la frecuencia y el porcentaje

Es importante recalcar que continuamos realizando análisis exploratorio de la base de datos y estamos respetando las instrucciones de cada subinciso, es por esto que ahora elaboraremos tablas que contengan información sobre la frecuencia y porcentaje de aquellas variables que lo requieren, como lo son: Complexión, Papás separados, Cantidad hermanos, Trabaja y Horas diarias trabajadas. 

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}

#Tabla de frecuencias de complexion
frecuency_as_df<-as.data.frame(tabyl(datos$Complexion, sort = TRUE))

names(frecuency_as_df) <- c("Complexion","Frecuencia","Porcentaje")
final_constitution_frecuency<-frecuency_as_df[-c(1),]

final_constitution_frecuency
```

A partir de esta tabla podemos inferir que el 56% de las personas que se encuentran registradas en la base de datos tienen un cuerpo mesomorfo, es decir, su complexión es intermedia (ni muy delgada ni muy gruesa) y se caracterizan por tener un metabolismo regular.

Por otro lado se observa que el 36% de las observaciones tienen un cuerpo endomorfo, lo que significa que desgraciadamente sufren de sobrepeso.

Y aquellos con complexión ectomorfa (muy delgada) solo cubren el 8% de nuestras observaciones.

En conclusión se puede decir que más de la mitad de las personas se encuentran saludables y con una buena complexión, sin embargo el 44% restante debería acudir con un médico y estar al pendiente de su salud.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
#Tabla de frecuencias de papás_separados
frecuency_as_df<-as.data.frame(tabyl(datos$Papas_separados, sort = TRUE))

names(frecuency_as_df) <- c("Papás_separados","Frecuencia","Porcentaje")
final_separated_parents_frecuency<-frecuency_as_df[-c(2),]

final_separated_parents_frecuency
```

Sabemos que solo el 20% de los individuos registrados en la base de datos tiene padres separados, esto puede ser por diversas razones, las cuales desconocemos. Esta información puede ser útil para evaluar si el desempeño de una persona o nivel de carga de trabajo se ve influenciado por la relación de sus progenitores.


```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
#Tabla de frecuencias de cantidad_hermanos
frecuency_as_df<-as.data.frame(tabyl(datos$Cantidad_hermanos, sort = TRUE))

names(frecuency_as_df) <- c("Cantidad_hermanos","Frecuencia","Porcentaje")
final_number_of_brothers_frecuency<-frecuency_as_df[-c(7),]

final_number_of_brothers_frecuency
```

Con esta tabla se puede inferir que en esta base de datos predominan las observaciones con menos de 3 hermanos, mientras que aquellas observaciones con más de 3 hermanos son menos frecuentes.
El número de hermanos es un dato relevante para estudiar si el nivel socioeconómico de una persona se ve afectado por la cantidad de hijos que deciden criar sus padres.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
#Tabla de frecuencias de los que trabajan
frecuency_as_df<-as.data.frame(tabyl(datos$Trabaja, sort = TRUE))

names(frecuency_as_df) <- c("Trabaja","Frecuencia","Porcentaje")
final_iswork_frecuency<-frecuency_as_df[-c(3),]

final_iswork_frecuency
```

De igual forma es importante destacar que el 72% de las personas registradas en la base de datos sí trabaja, mientras que el 28% restante no lo hace, esto puede ser debido a que esos individuos son menores de edad o por factores que no son posibles determinar con la información otorgada por la base de datos.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
#Filtro de las observaciones que SI trabajan

trabajan <- filter(datos, Trabaja == "SI")

#Tabla de frecuencias de horas trabajadas x dia

frecuency_as_df<-as.data.frame(tabyl(trabajan$Horas_trab_dia, sort = TRUE))

names(frecuency_as_df) <- c("Horas_trabajo x dia","Frecuencia","Porcentaje")
final_worktime_per_day_frecuency<-frecuency_as_df[-c(8),]

final_worktime_per_day_frecuency

```

Con esta tabla podemos observar que las personas registradas en la base de datos que sí trabajan, lo hacen en promedio entre 1 a 7 horas al día y que el 25% de ellas prefiere laborar medio día (4 horas).


Continuando con el análisis, es momento de enfocarnos en las otras variables de la base de datos (Peso, Estatura, Edad, Horas trabajadas al mes y Salario mensual), aquellas que no colocamos en tablas pues la frecuencia de sus datos es de 1 y sus tablas de frecuencia y porcentajes resultaban poco agradables visualmente.

Lo que haremos con ellas es visualizar un resumen de las mismas.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
summary(datos$Peso)
sd(datos$Peso)

```

Podemos notar que en la base de datos el peso más bajo registrado es de 37.85 Kg (que es probable que corresponda a una persona con complexión ectomorfa) y el máximo es de 75.95 Kg. Además, existe una desviación estándar de 8.31 Kg en el peso de las observaciones.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
summary(datos$Estatura)
sd(datos$Estatura)

```

La persona con la menor estatura de la base de datos mide 141 cm, mientras que la más alta mide 180 cm. Se cuenta con una desviación estándar de 11.13 cm, lo que significa que hay mucha variación en las estaturas registradas.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
summary(datos$Edad)
sd(datos$Edad)

```

La persona más joven de la base de datos tiene 2 años de edad, mientras que la más grande tiene 33 años. La variable de Edad tiene una desviación estándar de 5 años.


```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
summary(trabajan$Horas_trab_mes)
sd(trabajan$Horas_trab_mes)

```

El mínimo de horas laboradas mensualmente es de 1 y el máximo registrado es de 9 horas. Se cuenta con una desviación estándar de casi 3 horas.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
summary(trabajan$Salario_mes)
sd(trabajan$Salario_mes)

```

El salario mensual mínimo de las observaciones registradas es de 1 peso, mientras que el máximo es de 23 pesos. La variable de salario al mes tiene una desviación estándar de 6.58 pesos, que tomando en cuenta nuestro rango de valores (1 - 23) se puede decir que es muy amplia.


## 1.3 Hagan un gráfico donde pueda explicar las variables dependientes.


```{r, echo=TRUE,warning=FALSE,tidy=TRUE, message=FALSE}
numericas <- datos %>% dplyr::select(Peso, Estatura, Edad,Horas_trab_dia, 
                                     Horas_trab_mes, Salario_mes)
ggpairs(numericas, title = "Variables numéricas dependientes")
```

Con esta gráfica podemos notar que la correlación entre las variables numéricas que conforman la base de datos es, en su mayoría, débil. En el caso del salario mensual con respecto a la variable de horas laboradas al mes se observa una correlación de 0.57, la cual es la más alta vista en la gráfica, 

# 1.4 Construyan diferentes gráficos donde puedan mostrar patrones y detalles de la base de datos con respecto a una nueva variable que podemos denominar: “categoría”


```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
# Número de personas por categoría
cantidad_personas <- c(sum(with(datos, Edad < 18.0)), 
                       sum(with(datos, Edad >=18.0 & Edad<=25.0)),
                       sum(with(datos, Edad > 25.0)))

dist_personas <- data.frame(Categoria=c("Junior", "Señor", "Mayor"), Cantidad=cantidad_personas)
ggplot(dist_personas, aes(x=Categoria, y=Cantidad)) + geom_bar(stat = "identity") + coord_flip() + labs(title="Distribución de personas por categoría") +
  geom_text(aes(y=Cantidad, label=cantidad_personas), vjust=1.5, color="white", size=3.5, hjust="right")
```

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
# Salario mensual de una persona dependiendo su categoría
trabajan <- filter(datos, Trabaja == "SI")

trabajan$Categoria[trabajan$Edad < 18] = "Junior"
trabajan$Categoria[trabajan$Edad >= 18 & trabajan$Edad<=25] = "Senior"
trabajan$Categoria[trabajan$Edad > 25] = "Mayor"

Observaciones <-c(1:36)

ggplot(trabajan, aes(x= Observaciones, y=Salario_mes, col=Categoria)) + 
  labs(title="Salario mensual de una persona dependiendo su categoría") + geom_line()
```


```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
# Número de personas con padres separados por categoría
datos$Categoria[datos$Edad < 18] = "Junior"
datos$Categoria[datos$Edad >= 18 & datos$Edad<=25] = "Senior"
datos$Categoria[datos$Edad > 25] = "Mayor"

ggplot(datos, aes(Categoria, ..count..)) + geom_bar(aes(fill = Papas_separados), position = "dodge")
```

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
# Personas que trabajan dependiendo su categoría
ggplot(datos, aes(Categoria, ..count..)) + geom_bar(aes(fill = Trabaja), position = "dodge")
```


# 2. Metodo de reducción de dimensión (PCA)

Antes de aplicar el método PCA para reducir la dimensión de un conjunto de datos, vamos a re


## 2.1 PCA base de datos PCA1
Para esta base de datos decidimos aplicar el modelo PCA
Como primer paso sacamos el porcentaje de varianza y la graficamos.
Una vez ya obtenido este punto proseguimos a graficar la correlación.
```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
df2<-read.csv("Ejercicio 2.1.csv", sep = ",")
res.pca <- PCA(df2[,-1], graph = F)

eig.val <- get_eigenvalue(res.pca)
eig.val
fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 90))

var <- get_pca_var(res.pca)

corrplot(var$cos2, is.corr=FALSE)
```

Proseguimos a graficar las variables y de igual manera graficamos las observaciones.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
fviz_pca_var(res.pca, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE # Avoid text overlapping
)

fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")
)

fviz_pca_ind(res.pca,
             geom.ind = "point", # show points only (nbut not "text")
             col.ind =df2$tipo, # color by groups
             palette = c("#00AFBB", "#E7B800", "#FC4E07"),
             addEllipses = TRUE, # Concentration ellipses
             legend.title = "Groups")
```
El modelo logró agrupar correctamente las observaciones de la base de datos.

## 2.2 PCA base de datos PCA2
Aplicamos el modelo PCA para obtener el numero óptimo de componentes principales, debido que el conjunto de datos contaba más de 200 variables.
Graficando la varianza acumulada podremos obtener lo siguiente:
```{r, echo=TRUE,tidy=TRUE}
url<-"http://web.stanford.edu/~hastie/ElemStatLearn//datasets/zip.digits/train.2"
data<-read.csv(url)

data.pca = scale(data)
data.pca <- prcomp(data, center = F, scale=T)
prop_varianza <- data.pca$sdev^2 / sum(data.pca$sdev^2)
prop_varianza_acum <- cumsum(prop_varianza)

ggplot(data = data.frame(prop_varianza, pc = 1:256),
       aes(x = pc, y = prop_varianza)) +
  xlim(0, 50) +
  geom_col(width = 0.3) +
  scale_y_continuous(limits = c(0,.5)) +
  theme_bw() +
  labs(x = "Componente principal",
       y = "Prop. de varianza explicada")

ggplot(data = data.frame(prop_varianza_acum, pc = 1:256),
       aes(x = pc, y = prop_varianza_acum, group = 1)) +
  geom_point() +
  geom_line() +
  geom_label(aes(label = round(prop_varianza_acum,2))) +
  theme_bw() +
  labs(x = "Componente principal",
       y = "Prop. varianza explicada acumulada")
```

Una vez viendo la representación grafica podemos concluir que un numero óptimo de dimenciones para esta base de datos sería aproximadamente 10.

# 3. Metodo de reducción de dimensión (LDA)

## 3.1 Exploren los datos y encuentren gráficamente sus tendencias y sus posibles correlaciones en gráficos 2D y 3D.

Graficamos los datos y sus correlaciones mediante un modelo 2D y 3D.En la grafica 2D podemos apreciar que las variables tienen una distribución normal y la relación entre el largo y el ancho generan agrupaciones las cuales están bien definidos.

```{r, echo=TRUE,tidy=TRUE}
datos<-read.csv("Ejercicio 3.csv", sep = ",")
p1 <- ggplot(data = datos, aes(x = largo, fill = tipo)) +
  geom_histogram(position = "identity", alpha = 0.5)
p2 <- ggplot(data = datos, aes(x = ancho, fill = tipo)) +
  geom_histogram(position = "identity", alpha = 0.5)
p3 <- ggplot(data = datos, aes(x = alto, fill = tipo)) +
  geom_histogram(position = "identity", alpha = 0.5)
ggarrange(p1, p2, p3, nrow = 3, common.legend = TRUE)
pairs(x = datos[, c("largo","ancho","alto")],
      col = c("blue", "green","red")[datos$tipo], pch = 19,main="Grafica Correlacion 2D")

scatterplot3d(datos$largo, datos$ancho, datos$alto,
              color = c("firebrick", "green","blue")[datos$tipo], pch = 19,
              grid = TRUE, xlab = "largo", ylab = "ancho",
              zlab = "alto", angle = 65, cex.axis = 0.6,main = "Visualizacion 3D")
legend("topleft",
       bty = "n", cex = 0.8,
       title = "tipo",
       c("rectangular", "cuadrada", "circular"), fill = c("firebrick", "green","blue"))
```

## 3.2 Realicen la verificación de supuestos.

Aplicamos las técnicas vistas en clase para evaluar la normalidad multivariable y para identificar outlayers que influyen en el comportamiento los datos, mediante los test Mardia, Henze-Zirkler y Royston

```{r, echo=TRUE,tidy=TRUE}
datos_tidy <- melt(datos, value.name = "valor")
kable(datos_tidy %>% group_by(tipo, variable) %>% summarise(p_value_Shapiro.test =
                                                              shapiro.test(valor)$p.value))
outliers <- mvn(data = datos[,-1], mvnTest = "hz", multivariateOutlierMethod = "quan")
royston_test <- mvn(data = datos[,-1], mvnTest = "royston", multivariatePlot = "qq")
royston_test$multivariateNormality
hz_test <- mvn(data = datos[,-1], mvnTest = "hz")
hz_test$multivariateNormality
boxM(data = datos[,2:4], grouping = datos[,1])
```

## 3.3 Apliquen el método LDA o QDA según consideren conveniente paso a paso.

Para esta base de datos decidimos aplicar el modelo LDA, ya que los datos como vimos anteriormente tienen una distribución normal.

```{r, echo=TRUE,tidy=TRUE}
modelo_lda <- lda(formula = tipo ~ largo + ancho + alto,
                  data = datos)
```

## 3.4 Grafiquen como quedaría la clasificación y calculen su error para nuevos datos.

Al aplicar el modelo de predicción para una nueva observación y al evaluarla nos dio un error del 0% lo cual indica que el modelo funciona correctamente.

```{r, echo=TRUE,tidy=TRUE}
nuevas_observaciones <- data.frame(largo = 30, ancho = 15,
                                   alto = 20)
predict(object = modelo_lda, newdata = nuevas_observaciones)
predicciones <- predict(object = modelo_lda, newdata = datos[, -1],
                        method = "predictive")
table(datos$tipo, predicciones$class,
      dnn = c("Clase real", "Clase predicha"))
trainig_error <- mean(datos$especie != predicciones$class) * 100
paste("trainig_error=", trainig_error, "%")
with(datos, {
  s3d <- scatterplot3d(largo, ancho, alto,
                       color = c("firebrick", "green","blue")[datos$tipo],
                       pch = 19, grid = TRUE, xlab = "largo", ylab = "ancho",
                       zlab = "altura", angle = 65, cex.axis = 0.6,main = "Clasificacion de los Nuevos Datos")
  
  s3d.coords <- s3d$xyz.convert(largo, ancho, alto)
  # convierte coordenadas 3D en proyecciones 2D
  
  legend("topleft", 
         bty = "n", cex = 0.8,
         title = "Tipo",
         c("largo", "ancho","alto"), fill = c("firebrick", "green","blue"))
})
```

Empleando las mismas observaciones con las que se ha generado el modelo discriminante (trainig data), la precisión de clasificación es del 100%.