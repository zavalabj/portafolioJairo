---
title: "Corrección Examen Parcial 1"
author: "ByteMiners"
date: "09/03/2020"
output:
  pdf_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Análisis exploratorio de datos
Las librerías usadas en el examen fueron las siguientes:
```{r, echo=TRUE,message=FALSE,warning=FALSE}
library(ggplot2)
library(cowplot)
library(dplyr)
library(grid)
library(FactoMineR)
library(factoextra)
library(corrplot)
library(ggpubr)
library(scatterplot3d)
library(reshape2)
library(knitr)
library(MVN)
library(biotools)
library(klaR)
library(janitor)
library(psych)
library(gridExtra)
library(GGally)
library(funModeling)
```
# 1. Exploración y visualización de variables

## 1.1 Explorar la base de datos, realizar los cambios necesarios y convenientes para poder trabajar con ella.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE} 
df <- read.csv("Ejercicio 1.1.csv", sep = ",")
df
```

Como podemos observar esta base de datos cuenta con 10 variables o características, las cuales son: Peso, Estatura, Edad, Complexión, Papas separados, Cantidad hermanos, Trabaja, Horas trabajadas por día, Horas trabajadas por mes y salario mensual.

Además, es posible notar que se conforma de 50 observaciones, sin embargo es necesario reordenar los datos, ya que cada observación debe estar contenida en una fila y no en una columna (como se nos fue otorgada la información inicialmente).

Lo primero que realizamos fue la trasposición de las columnas del dataframe para que cada observación fuera vista en una fila y no por columnas como inicialmente estaba acomodada la información.

Otra cosa que debemos arreglar son las observaciones en donde el valor de la característica "Trabaja" es "NO", debido a que contienen variables relacionadas con la anteriormente mencionada (las cuales son: "Horas_trab_dia", "Horas_trab_mes" y "Salario_mes") y que se encuentran vacías. Esto último puede afectar nuestro análisis, es por eso que decidimos colocar ceros en aquellos campos en donde existen NA.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE} 

#Trasponer las columnas del df para que las características de cada observacion estén ordenadas por columnas 
df_transposed <- as.data.frame(t(as.matrix(df)))

# Colocar nombre a las columnas y eliminar la primera fila 
names(df_transposed) <- c("Peso","Estatura","Edad","Complexion","Papas_separados","Cantidad_hermanos",
                                  "Trabaja", "Horas_trab_dia", "Horas_trab_mes", "Salario_mes")

datos <- df_transposed[-c(1),] 

# Reemplazar valores NA con 0 solo en las columnas seleccionadas
vars_to_replace=c("Horas_trab_dia", "Horas_trab_mes", "Salario_mes")

datos = datos %>% mutate_at(.vars=vars_to_replace, .funs = funs(ifelse(is.na(.), 0, .)))

datos
```

Para continuar con el análisis exploratorio, es momento de distinguir el tipo de dato correspondiente para cada variable que conforma esta base de datos. Esto con el fin de poder determinar cómo explorar la base de datos y si es necesario modificar algún tipo de dato para realizar tablas o gráficas relevantes.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE} 
# Analizar la base de datos
df_status(datos)
```

Existen 7 variables de tipo "factor" el cual es usado para variables categóricas o nominales, lo curioso es que de esas 7 variables, solo 3 (Complexión, Papas separados y Trabaja) son categóricas y las restantes (Peso, Estatura, Edad y Cantidad hermanos) deberían ser numéricas.

Para trabajar con las variables Horas_trab_dia, Horas_trab_mes y Salario_mes también se debe realizar un ajuste a su tipo de dato porque, como se pudo observar, al momento de cambiar los NA por ceros los valores almacenados en esos campos fueron alterados.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE} 
# Convertir en valores númericos aquellos almacenados en las columnas seleccionadas
vars_to_replace=c("Peso", "Estatura", "Edad", "Cantidad_hermanos")

datos[vars_to_replace] <- lapply(datos[vars_to_replace], function(x) as.numeric(as.character(x)))

# Cargamos csv con los valores previamente almacenados en los campos Horas_trab_dia, Horas_trab_mes y Salario_mes
df2 <- read.csv("Ejercicio 1.csv", sep = ",")

#Trasponer las columnas del df para que las características de cada observacion estén ordenadas por columnas 
df2 <- as.data.frame(t(as.matrix(df2)))

# Colocar nombre a las columnas y eliminar la primera fila 
names(df2) <- c("Peso","Estatura","Edad","Complexion","Papas_separados","Cantidad_hermanos",
                                  "Trabaja", "Horas_trab_dia", "Horas_trab_mes", "Salario_mes")

df2 <- df2[-c(1),] 

datos$Horas_trab_dia <- as.numeric(as.numeric_version(df2$Horas_trab_dia))
datos$Horas_trab_mes <- as.numeric(as.numeric_version(df2$Horas_trab_mes))
datos$Salario_mes <- as.numeric(as.numeric_version(df2$Salario_mes))

status(datos)
```

Otra corrección que se debe realizar es hacer función piso a los valores de Edad, ya que estos fueron ingresados como decimales y eso es erróneo porque deben ser considerados como discretos.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
datos['Edad'] <- floor(datos['Edad'])

```

## 1.2  Expliquen por medio de tablas esta información. En la tabla debe aparecer simultáneamente la frecuencia y el porcentaje

Es importante recalcar que continuamos realizando análisis exploratorio de la base de datos y estamos respetando las instrucciones de cada subinciso, es por esto que ahora elaboraremos tablas que contengan información sobre la frecuencia y porcentaje de aquellas variables que lo requieren, como lo son: Complexión, Papás separados, Cantidad hermanos, Trabaja y Horas diarias trabajadas. 

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}

#Tabla de frecuencias de complexion
frequency_as_df<-as.data.frame(tabyl(datos$Complexion, sort = TRUE))

names(frequency_as_df) <- c("Complexion","Frecuencia","Porcentaje")
final_constitution_frequency<-frequency_as_df[-c(1),]

final_constitution_frequency
```

A partir de esta tabla podemos inferir que el 56% de las personas que se encuentran registradas en la base de datos tienen un cuerpo mesomorfo, es decir, su complexión es intermedia (ni muy delgada ni muy gruesa) y se caracterizan por tener un metabolismo regular.

Por otro lado se observa que el 36% de las observaciones tienen un cuerpo endomorfo, lo que significa que desgraciadamente sufren de sobrepeso.

Y aquellos con complexión ectomorfa (muy delgada) solo cubren el 8% de nuestras observaciones.

En conclusión se puede decir que más de la mitad de las personas se encuentran saludables y con una buena complexión, sin embargo el 44% restante debería acudir con un médico y estar al pendiente de su salud.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
#Tabla de frecuencias de papás_separados
frequency_as_df<-as.data.frame(tabyl(datos$Papas_separados, sort = TRUE))

names(frequency_as_df) <- c("Papás_separados","Frecuencia","Porcentaje")
final_separated_parents_frequency<-frequency_as_df[-c(2),]

final_separated_parents_frequency
```

Sabemos que solo el 20% de los individuos registrados en la base de datos tiene padres separados, esto puede ser por diversas razones, las cuales desconocemos. Esta información puede ser útil para evaluar si el desempeño de una persona o nivel de carga de trabajo se ve influenciado por la relación de sus progenitores.


```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
#Tabla de frecuencias de cantidad_hermanos
frequency_as_df<-as.data.frame(tabyl(datos$Cantidad_hermanos, sort = TRUE))

names(frequency_as_df) <- c("Cantidad_hermanos","Frecuencia","Porcentaje")
final_siblings_frequency<-frequency_as_df[-c(7),]

final_siblings_frequency
```

Con esta tabla se puede inferir que en esta base de datos predominan las observaciones con menos de 3 hermanos, mientras que aquellas observaciones con más de 3 hermanos son menos frecuentes.
El número de hermanos es un dato relevante para estudiar si el nivel socioeconómico de una persona se ve afectado por la cantidad de hijos que deciden criar sus padres.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
#Tabla de frecuencias de los que trabajan
frequency_as_df<-as.data.frame(tabyl(datos$Trabaja, sort = TRUE))

names(frequency_as_df) <- c("Trabaja","Frecuencia","Porcentaje")
final_iswork_frequency<-frequency_as_df[-c(3),]

final_iswork_frequency
```

De igual forma es importante destacar que el 72% de las personas registradas en la base de datos sí trabaja, mientras que el 28% restante no lo hace, esto puede ser debido a que esos individuos son menores de edad o por factores que no son posibles determinar con la información otorgada por la base de datos.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
#Filtro de las observaciones que SI trabajan

trabajan <- filter(datos, Trabaja == "SI")

#Tabla de frecuencias de horas trabajadas x dia

frequency_as_df<-as.data.frame(tabyl(trabajan$Horas_trab_dia, sort = TRUE))

names(frequency_as_df) <- c("Horas_trabajo x dia","Frecuencia","Porcentaje")

frequency_as_df
```

Con esta tabla podemos observar que las personas registradas en la base de datos que sí trabajan, lo hacen en promedio entre 1 a 7 horas al día y que el 25% de ellas prefiere laborar medio día (4 horas).


Continuando con el análisis, es momento de enfocarnos en las otras variables de la base de datos (Peso, Estatura, Edad, Horas trabajadas al mes y Salario mensual), aquellas que no colocamos en tablas pues la frecuencia de sus datos es de 1 y sus tablas de frecuencia y porcentajes resultaban poco agradables visualmente.

Lo que haremos con ellas es visualizar un resumen de las mismas.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
summary(datos$Peso)
sd(datos$Peso)

```

Podemos notar que en la base de datos el peso más bajo registrado es de 37.85 Kg (que es probable que corresponda a una persona con complexión ectomorfa) y el máximo es de 75.95 Kg. Además, existe una desviación estándar de 8.31 Kg en el peso de las observaciones.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
summary(datos$Estatura)
sd(datos$Estatura)

```

La persona con la menor estatura de la base de datos mide 141 cm, mientras que la más alta mide 180 cm. Se cuenta con una desviación estándar de 11.13 cm, lo que significa que hay mucha variación en las estaturas registradas.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
summary(datos$Edad)
sd(datos$Edad)

```

La persona más joven de la base de datos tiene 2 años de edad, mientras que la más grande tiene 33 años. La variable de Edad tiene una desviación estándar de 5 años.


```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
summary(trabajan$Horas_trab_mes)
sd(trabajan$Horas_trab_mes)

```

El mínimo de horas laboradas mensualmente es de 20 y el máximo registrado es de 80 horas. Se cuenta con una desviación estándar de casi 20 horas, la cual es muy amplia.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
summary(trabajan$Salario_mes)
sd(trabajan$Salario_mes)

```

El salario mensual mínimo de las observaciones registradas es de 800 pesos, mientras que el máximo es de 12000 pesos. La variable de salario al mes tiene una desviación estándar de 2863.7 pesos, que tomando en cuenta nuestro rango de valores (800 - 12000) se puede decir que es muy amplia.

El promedio de 3958 pesos es probable que esté influenciado por datos atípicos


## 1.3 Hagan un gráfico donde pueda explicar las variables dependientes.


```{r, echo=TRUE,warning=FALSE,tidy=TRUE, message=FALSE}
numericas <- datos %>% dplyr::select(Peso, Estatura, Edad,Horas_trab_dia, 
                                     Horas_trab_mes, Salario_mes)
ggpairs(numericas, title = "Variables numéricas dependientes")
```

Con esta gráfica podemos notar que la correlación entre las variables numéricas que conforman la base de datos es, en su mayoría, un tanto débil. En el caso del salario mensual con respecto a la variable de horas laboradas al mes se observa una correlación de 0.799, la cual es de las más altas vistas en la gráfica, pero no es sorpresa porque sabemos que en ocasiones el sueldo mensual depende de las horas que se laboran en ese periodo, como es en el caso de esta base de datos.

Y otra correlación que desde un principio era evidente que sería muy fuerte es la de las horas trabajadas al día con respecto a las horas trabajadas al mes, la cual tien un valor de 0.945.

# 1.4 Construyan diferentes gráficos donde puedan mostrar patrones y detalles de la base de datos con respecto a una nueva variable que podemos denominar: “categoría”


```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
# Número de personas por categoría
cantidad_personas <- c(sum(with(datos, Edad < 18.0)), 
                       sum(with(datos, Edad >=18.0 & Edad<=25.0)),
                       sum(with(datos, Edad > 25.0)))

dist_personas <- data.frame(Categoria=c("Junior", "Señor", "Mayor"), Cantidad=cantidad_personas)
ggplot(dist_personas, aes(x=Categoria, y=Cantidad)) + geom_bar(stat = "identity") + coord_flip() + labs(title="Distribución de personas por categoría") +
  geom_text(aes(y=Cantidad, label=cantidad_personas), vjust=1.5, color="white", size=3.5, hjust="right")
```

Como se puede observar el grupo con más personas es el de "Señor", lo que significa que más del 50% de las personas registradas en la base de datos tienen edades entre 18 a 25 años. En cambio, en las otras 2 categorías existen 22 personas, 11 respectivamente.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
# Personas que trabajan dependiendo su categoría
ggplot(datos, aes(Categoria, ..count..)) + geom_bar(aes(fill = Trabaja), position = "dodge")
```

Como era de esperarse, en la categoría junior hay más personas que no trabajan. En el caso de la categoría Señor las personas que sí trabajan triplican a la cantidad que no hace.


```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
# Salario mensual de una persona dependiendo su categoría
trabajan <- filter(datos, Trabaja == "SI")

trabajan$Categoria[trabajan$Edad < 18] = "Junior"
trabajan$Categoria[trabajan$Edad >= 18 & trabajan$Edad<=25] = "Senior"
trabajan$Categoria[trabajan$Edad > 25] = "Mayor"

Observaciones <-c(1:36)

ggplot(trabajan, aes(x= Observaciones, y=Salario_mes, col=Categoria)) + 
  labs(title="Salario mensual de una persona dependiendo su categoría") + geom_line()+
 geom_point(aes(x = Observaciones, y = Salario_mes ))
```

Para realizar esta gráfica omitimos aquellas personas que no trabajan de cada categoría. Lo anterior hace posible notar que el grupo con salarios más bajos es el de Junior, mientras que en el de Mayor existe un dato atipico cuyo salario asciende a los 12000 pesos cuando el resto de observaciones de dicha categoría no superan los 5500 pesos.

Se podría decir que la categoría con mejores salarios es la de Señor, pues supera (en algunos casos) a las otras dos agrupaciones. 


```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
# Número de personas con padres separados por categoría
datos$Categoria[datos$Edad < 18] = "Junior"
datos$Categoria[datos$Edad >= 18 & datos$Edad<=25] = "Senior"
datos$Categoria[datos$Edad > 25] = "Mayor"

ggplot(datos, aes(Categoria, ..count..)) + geom_bar(aes(fill = Papas_separados), position = "dodge")
```

Con esta gráfica es posible notar que todas las categorías tienen algo en común, y eso es que predominan personas cuyos padres no están separados.

# 2. Metodo de reducción de dimensión (PCA)

Antes de aplicar el método PCA para reducir la dimensión de un conjunto de datos, vamos a realizar un breve análisis exploratorio a la base de datos del inciso 2.1.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
df2<-read.csv("Ejercicio 2.1.csv", sep = ",")
status(df2)

```

La base de datos contiene 4 variables, las cuales son: tipo, largo, ancho y alto. Además, cabe destacar que esta constituida por 300 observaciones.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
frecuency_as_df<-as.data.frame(tabyl(df2$tipo, sort = TRUE))

names(frecuency_as_df) <- c("Tipo","Frecuencia","Porcentaje")

frecuency_as_df

```

Como podemos observar existen 3 tipos: cama, mueble y silla. Y de cada grupo hay 100 observaciones, es decir su porcentaje de frecuencia es el mismo, 33%.

## 2.1 PCA base de datos PCA1
Para esta base de datos decidimos aplicar el modelo PCA
Como primer paso sacamos el porcentaje de varianza y la graficamos.
Una vez ya obtenido este punto proseguimos a graficar la correlación.
```{r, echo=TRUE,warning=FALSE,tidy=TRUE}

res.pca <- PCA(df2[,-1], graph = F)

eig.val <- get_eigenvalue(res.pca)
eig.val
fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 90))

var <- get_pca_var(res.pca)

corrplot(var$cos2, is.corr=FALSE)
```

Proseguimos a graficar las variables y de igual manera graficamos las observaciones.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
fviz_pca_var(res.pca, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE # Avoid text overlapping
)

fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")
)

fviz_pca_ind(res.pca,
             geom.ind = "point", # show points only (nbut not "text")
             col.ind =df2$tipo, # color by groups
             palette = c("#00AFBB", "#E7B800", "#FC4E07"),
             addEllipses = TRUE, # Concentration ellipses
             legend.title = "Groups")
```
El modelo logró agrupar correctamente las observaciones de la base de datos.

## 2.2 PCA base de datos PCA2

Antes de aplicar el método PCA para reducir la dimensión de un conjunto de datos, vamos a realizar un breve análisis exploratorio a la base de datos del inciso 2.1.

```{r, echo=TRUE,tidy=TRUE, warning=FALSE}
url<-"http://web.stanford.edu/~hastie/ElemStatLearn//datasets/zip.digits/train.2"
data<-read.csv(url)

```

Esta base de datos es más grande que las anteriores, cuenta con 730 observaciones y 256 variables, mismas que se pretende reducir a través del modelo PCA, con el cual obtendremos el número óptimo de componentes principales que describen la mayor cantidad de información del conjunto de datos sin necesidad de utilizar una enorme cantidad de predictores. 

Graficando la varianza acumulada, y guiados por el método del codo, podremos obtener lo siguiente:

```{r, echo=TRUE,tidy=TRUE, warning=FALSE}

data.pca = scale(data)
data.pca <- prcomp(data, center = F, scale=T)
prop_varianza <- data.pca$sdev^2 / sum(data.pca$sdev^2)
prop_varianza_acum <- cumsum(prop_varianza)

ggplot(data = data.frame(prop_varianza, pc = 1:256),
       aes(x = pc, y = prop_varianza)) +
  xlim(0, 25) +
  geom_col(width = 0.3) +
  scale_y_continuous(limits = c(0,.5)) +
  theme_bw() +
  labs(x = "Componente principal",
       y = "Prop. de varianza explicada")

ggplot(data = data.frame(prop_varianza_acum, pc = 1:256),
       aes(x = pc, y = prop_varianza_acum, group = 1)) +
  xlim(0,18)+
  geom_point() +
  geom_line() +
  geom_label(aes(label = round(prop_varianza_acum,2))) +
  theme_bw() +
  labs(x = "Componente principal",
       y = "Prop. varianza explicada acumulada")
```

Guiados por la primera gráfica se podría pensar que el número más optimo es de 10 aproximadamente, sin embargo al observar la gráfica 2 es posible notar que un valor más acertado es 18, con el cual se describe un 82% de los datos originales y con lo cual estaríamos reduciendo bastante la dimensionalidad de la base de datos que inicialmente era de 256.

# 3. Metodo de reducción de dimensión (LDA)

Primeramente debemos realizar un breve análsis exploratorio de la base de datos para entender mejor la información con la que trabajaremos.
```{r, echo=TRUE,tidy=TRUE}
datos<-read.csv("Ejercicio 3.csv", sep = ",")
df_status(datos)
```

Es posible notar que, al igual que la base de datos del inciso 2.1, contiene 4 variables. Las cuales son: tipo, largo, ancho y alto.
También se observó que cuenta con 300 observaciones.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
frecuency_as_df<-as.data.frame(tabyl(datos$tipo, sort = TRUE))

names(frecuency_as_df) <- c("Tipo","Frecuencia","Porcentaje")

frecuency_as_df

```

Tipo es una variable nominal que contiene 3 valores: circular, cuadrada y rectangular. Dichos tipos o categorias se encuentran presentes en la base de datos por partes iguales, es decir, existen 100 observaciones que correspondern a cada tipo.


## 3.1 Exploren los datos y encuentren gráficamente sus tendencias y sus posibles correlaciones en gráficos 2D y 3D.

Graficamos los datos y sus correlaciones mediante un modelo 2D y 3D.En la grafica 2D podemos apreciar que las variables tienen una distribución normal y la relación entre el largo y el ancho generan agrupaciones las cuales están bien definidos.

```{r, echo=TRUE,tidy=TRUE, warning=FALSE, message=FALSE}
datos<-read.csv("Ejercicio 3.csv", sep = ",")
p1 <- ggplot(data = datos, aes(x = largo, fill = tipo)) +
  geom_histogram(position = "identity", alpha = 0.5)
p2 <- ggplot(data = datos, aes(x = ancho, fill = tipo)) +
  geom_histogram(position = "identity", alpha = 0.5)
p3 <- ggplot(data = datos, aes(x = alto, fill = tipo)) +
  geom_histogram(position = "identity", alpha = 0.5)
ggarrange(p1, p2, p3, nrow = 3, common.legend = TRUE)
pairs(x = datos[, c("largo","ancho","alto")],
      col = c("blue", "green","red")[datos$tipo], pch = 19,main="Grafica Correlacion 2D")
```
 
 Es importante destacar en las gráficas de dispersiones se puede apreciar que la relación de largo con respecto al ancho (y viceversa) crea unos gráficos en donde se ven perfectamente la separación de los tipos (circular, rectangular y cuadrada).

```{r, echo=TRUE,tidy=TRUE}

scatterplot3d(datos$largo, datos$ancho, datos$alto,
              color = c("firebrick", "green","blue")[datos$tipo], pch = 19,
              grid = TRUE, xlab = "largo", ylab = "ancho",
              zlab = "alto", angle = 65, cex.axis = 0.6,main = "Visualización 3D")
legend("topleft",
       bty = "n", cex = 0.8,
       title = "tipo",
       c("rectangular", "cuadrada", "circular"), fill = c("firebrick", "green","blue"))
```

## 3.2 Realicen la verificación de supuestos.

Aplicamos las técnicas vistas en clase para evaluar la normalidad multivariable y para identificar outlayers que influyen en el comportamiento los datos, mediante los test Mardia, Henze-Zirkler y Royston

```{r, echo=TRUE,tidy=TRUE}
datos_tidy <- melt(datos, value.name = "valor")
kable(datos_tidy %>% group_by(tipo, variable) %>% summarise(p_value_Shapiro.test =
                                                              shapiro.test(valor)$p.value))
outliers <- mvn(data = datos[,-1], mvnTest = "hz", multivariateOutlierMethod = "quan")
royston_test <- mvn(data = datos[,-1], mvnTest = "royston", multivariatePlot = "qq")
royston_test$multivariateNormality
hz_test <- mvn(data = datos[,-1], mvnTest = "hz")
hz_test$multivariateNormality
boxM(data = datos[,2:4], grouping = datos[,1])
```

## 3.3 Apliquen el método LDA o QDA según consideren conveniente paso a paso.

Para esta base de datos decidimos aplicar el modelo LDA, ya que los datos como vimos anteriormente tienen una distribución normal.

```{r, echo=TRUE,tidy=TRUE}
modelo_lda <- lda(formula = tipo ~ largo + ancho + alto,
                  data = datos)
```

## 3.4 Grafiquen como quedaría la clasificación y calculen su error para nuevos datos.

Al aplicar el modelo de predicción para una nueva observación y al evaluarla nos dio un error del 0% lo cual indica que el modelo funciona correctamente.

```{r, echo=TRUE,tidy=TRUE}
nuevas_observaciones <- data.frame(largo = 30, ancho = 15,
                                   alto = 20)
predict(object = modelo_lda, newdata = nuevas_observaciones)
predicciones <- predict(object = modelo_lda, newdata = datos[, -1],
                        method = "predictive")
table(datos$tipo, predicciones$class,
      dnn = c("Clase real", "Clase predicha"))
trainig_error <- mean(datos$especie != predicciones$class) * 100
paste("trainig_error=", trainig_error, "%")
with(datos, {
  s3d <- scatterplot3d(largo, ancho, alto,
                       color = c("firebrick", "green","blue")[datos$tipo],
                       pch = 19, grid = TRUE, xlab = "largo", ylab = "ancho",
                       zlab = "altura", angle = 65, cex.axis = 0.6,main = "Clasificacion de los Nuevos Datos")
  
  s3d.coords <- s3d$xyz.convert(largo, ancho, alto)
  # convierte coordenadas 3D en proyecciones 2D
  
  legend("topleft", 
         bty = "n", cex = 0.8,
         title = "Tipo",
         c("largo", "ancho","alto"), fill = c("firebrick", "green","blue"))
})
```

Empleando las mismas observaciones con las que se ha generado el modelo discriminante (trainig data), la precisión de clasificación es del 100%.

# 4. Método de reducción de dimensión (CCA)

## 4.1 Exploren los datos y encuentren gráficamente sus tendencias y sus posibles correlaciones.

## 4.2 Apliquen el método CCA paso a paso encontrando todas la matrices de correlación.

## 4.3 Expliquen de forma gráfica como se relacionan estas variables

## 4.4 Construyan como quedaría una posible clasificación de estas observaciones según estas variables.


```{r, echo=TRUE,tidy=TRUE}
data<-read.csv("Ejercicio 4.csv", sep = ",")
ggpairs(data, title = "Deportistas")
dataa<-c("peso","altura","edad","ansiedad")
datos<-data[dataa]
cov(datos)
X <-datos[,(1:2)]
cov(X)
Y <-datos[,(3:4)]
cov(Y)
cov(X,Y)
cov(Y,X)
A <-solve(cov(X,X))%*%cov(X,Y)%*%solve(cov(Y,Y))%*%cov(Y,X)
B <-solve(cov(Y,Y))%*%cov(Y,X)%*%solve(cov(X,X))%*%cov(X,Y)
eigen(A)$values 
eigen(B)$values 
r <-sqrt(eigen(A)$values)
r
a <-eigen(A)$vector[,1]
b <-eigen(B)$vector[,1]
a
b
t(a)%*%cov(X,X)%*%a
mat_cor  <-matcor(datos,datos)
mat_cor
cca1<- cc(datos,datos)
cca1
img.matcor(mat_cor,type=2)
plt.cc(cca1,var.label=T)
cca2<- cca(datos,datos)
cca2
plot(cca2)
```
