---
title: "Examen Parcial 1"
author: "ByteMiners"
date: "23/02/2020"
output:
  pdf_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Análisis exploratorio de datos
```{r, echo=TRUE,message=FALSE,warning=FALSE}
library(ggplot2)
library(cowplot)
library(dplyr)
library(grid)
library(FactoMineR)
library(factoextra)
library(corrplot)
library(ggpubr)
library(scatterplot3d)
library(reshape2)
library(knitr)
library(MVN)
library(biotools)
library(klaR)
library(janitor)
library(psych)
library(gridExtra)
library(GGally)
```
# 1. Exploración y visualización de variables
## 1.1 Explorar la base de datos, realizar los cambios necesarios y convenientes para poder trabajar con ella.

Lo primero que realizamos fue la trasposicón de las columnas del dataframe para que cada observación fuera vista en una fila y no por columnas como inicialmente estaba acomodada la información.

Después optamos por eliminar aquellas observaciones que tuvieran valores nulos (NA).

```{r, echo=TRUE,warning=FALSE,tidy=TRUE} 
df <- read.csv("Ejercicio 1.csv", sep = ",")

#Trasponer las columnas del df para que las características de cada observacion estén ordenadas por columnas 
df_transposed <- as.data.frame(t(as.matrix(df)))

# Eliminar observaciones con valores NA
df_transposed_omitted<-na.omit(df_transposed)

# Colocar nombre a las columnas y eliminar la primera fila 
names(df_transposed_omitted) <- c("Peso","Estatura","Edad","Complexion","Papas_separados","Cantidad_hermanos",
                                  "Trabaja", "Horas_trab_dia", "Horas_trab_mes", "Salario_mes")

updated_data <- df_transposed_omitted[-c(1),] 
```
## 1.2  Expliquen por medio de tablas esta información. En la tabla debe aparecer simultáneamente la frecuencia y el porcentaje

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
#Tabla de frecuencias de peso
frecuency_as_df<-as.data.frame(tabyl(updated_data$Peso, sort = TRUE))

names(frecuency_as_df) <- c("Peso","Frecuencia","Porcentaje")
final_weight_frecuency<-frecuency_as_df[-c(51),]

final_weight_frecuency

#Tabla de frecuencias de estatura
frecuency_as_df<-as.data.frame(tabyl(updated_data$Estatura, sort = TRUE))

names(frecuency_as_df) <- c("Estatura","Frecuencia","Porcentaje")
final_height_frecuency<-frecuency_as_df[-c(51),]

final_height_frecuency

#Tabla de frecuencias de edad
frecuency_as_df<-as.data.frame(tabyl(updated_data$Edad, sort = TRUE))

names(frecuency_as_df) <- c("Edad","Frecuencia","Porcentaje")
final_age_frecuency<-frecuency_as_df[-c(51),]

final_age_frecuency


#Tabla de frecuencias de complexion
frecuency_as_df<-as.data.frame(tabyl(updated_data$Complexion, sort = TRUE))

names(frecuency_as_df) <- c("Complexion","Frecuencia","Porcentaje")
final_constitution_frecuency<-frecuency_as_df[-c(1),]

final_constitution_frecuency


#Tabla de frecuencias de papás_separados
frecuency_as_df<-as.data.frame(tabyl(updated_data$Papas_separados, sort = TRUE))

names(frecuency_as_df) <- c("Papás_separados","Frecuencia","Porcentaje")
final_separated_parents_frecuency<-frecuency_as_df[-c(2),]

final_separated_parents_frecuency

#Tabla de frecuencias de cantidad_hermanos
frecuency_as_df<-as.data.frame(tabyl(updated_data$Cantidad_hermanos, sort = TRUE))

names(frecuency_as_df) <- c("Cantidad_hermanos","Frecuencia","Porcentaje")
final_number_of_brothers_frecuency<-frecuency_as_df[-c(7),]

final_number_of_brothers_frecuency

#Tabla de frecuencias de los que trabajan
frecuency_as_df<-as.data.frame(tabyl(updated_data$Trabaja, sort = TRUE))

names(frecuency_as_df) <- c("Trabaja","Frecuencia","Porcentaje")
final_iswork_frecuency<-frecuency_as_df[-c(3),]

final_iswork_frecuency

#Tabla de frecuencias de horas trabajadas x dia
frecuency_as_df<-as.data.frame(tabyl(updated_data$Horas_trab_dia, sort = TRUE))

names(frecuency_as_df) <- c("Horas_trabajo x dia","Frecuencia","Porcentaje")
final_worktime_per_day_frecuency<-frecuency_as_df[-c(8),]

final_worktime_per_day_frecuency

#Tabla de frecuencias de horas trabajadas por mes
frecuency_as_df<-as.data.frame(tabyl(updated_data$Horas_trab_mes, sort = TRUE))

names(frecuency_as_df) <- c("Horas_trabajo x mes","Frecuencia","Porcentaje")
final_worktime_per_month_frecuency<-frecuency_as_df[-c(10),]

final_worktime_per_month_frecuency

#Tabla de frecuencias de salarios mensuales
frecuency_as_df<-as.data.frame(tabyl(updated_data$Salario_mes, sort = TRUE))

names(frecuency_as_df) <- c("Salario x mes","Frecuencia","Porcentaje")
final_salary_per_month_frecuency<-frecuency_as_df[-c(24),]

final_salary_per_month_frecuency
```

## 1.3 Hagan un gráfico donde pueda explicar las variables dependientes.

Seleccionamos como variable dependiente a la de salario mensual y como variables predictoras a las horas trabajadas al mes y las horas trabajadas por día. Aplicando el método de pearson obtuvimos el siguiente modelo: $9.1650+0.1895x+0.4341y$

Además, observando la correlación de las variables se puede notar que ésta es débil.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
datachida<-data.frame(cbind(updated_data$Horas_trab_dia,updated_data$Horas_trab_mes,updated_data$Salario_mes))
colnames(datachida) =c("Horas_trab_dia","Horas_trab_mes","Salario_mes")
round(cor(x = datachida, method = "pearson"), 3)
multi.hist(x = datachida, dcol = c("blue", "red"), dlty = c("dotted", "solid"))
ggpairs(datachida, lower = list(continuous = "smooth"),
        diag = list(continuous = "bar"), axisLabels = "none")
modelo <- lm(Salario_mes ~ Horas_trab_dia + Horas_trab_mes, data = datachida )
summary(modelo)
plot1 <- ggplot(data = datachida, aes(Horas_trab_mes, modelo$residuals)) +
    geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
    theme_bw()
plot2 <- ggplot(data = datachida, aes(Horas_trab_dia, modelo$residuals)) +
    geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
    theme_bw()
grid.arrange(plot1, plot2)
```

## 1.4 Construyan diferentes gráficos donde puedan mostrar patrones y detalles de la base de datos con respecto a una nueva variable que podemos denominar: “categoría”

Para este punto utilizamos la función de filter para obtener los registros que cumplieran con cada una de las categorías planteadas y seguidamente realizamos 3 gráficas: la primera para visualizar la cantidad de personas que existen por cada categoria, la segunda para apreciar el salario mensual promedio dependiendo de la categoria (junior, señor, mayor) y la tercera para ver la relación de personas con padres separados.

Con estas gráficas podemos concluir que existe un número más alto de personas mayores a 25 años y que la categoría con mejor salario mensual aproximadamente es la de "Señor", además de que el 29% de las personas pertenecientes a la categoría "Mayor" tiene padres separados.

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
# Número de personas por categoría
cantidad_personas <- c(sum(with(updated_data, as.numeric(Edad) < 18.0)), 
                       sum(with(updated_data, as.numeric(Edad)>=18.0 & as.numeric(Edad)<=25.0)),
                       sum(with(updated_data, as.numeric(Edad) > 25.0)))

dist_personas <- data.frame(Categoria=c("Junior", "Señor", "Mayor"), Cantidad=cantidad_personas)
ggplot(dist_personas, aes(x=Categoria, y=Cantidad)) + geom_bar(stat = "identity",fill=rainbow(3)) + coord_flip() + labs(title="Distribución de personas por categoría") +
  geom_text(aes(y=Cantidad, label=cantidad_personas), vjust=1.5, color="white", size=3.5, hjust="right")

# Salario mensual promedio de una persona dependiendo su categoría
junior <- filter(updated_data, as.numeric(Edad) <18.0)
senior <- filter(updated_data, as.numeric(Edad)>=18.0 & as.numeric(Edad)<=25.0)
mayor <- filter(updated_data, as.numeric(Edad) > 25.0)

prom_jun <- mean(as.numeric(as.numeric_version(junior$Salario_mes)))
prom_sen <- mean(as.numeric(as.numeric_version(senior$Salario_mes)))
prom_may <- mean(as.numeric(as.numeric_version(mayor$Salario_mes)))

Salario <- c(format(round(prom_jun, 2), nsmall = 2), 
                      format(round(prom_sen, 2), nsmall = 2),
                      format(round(prom_may, 2), nsmall = 2))

salario_trab <- data.frame(Categoria=c("Junior", "Señor", "Mayor"), Salario_Promedio = Salario)

ggplot(salario_trab, aes(x=Categoria, y=Salario)) + geom_bar(stat = "identity",fill=rainbow(3)) + coord_flip() + labs(title="Salario mensual promedio dependiendo la categoría")+
  geom_text(aes(y=Salario, label=Salario), vjust=1.5, color="white", size=3.5, hjust="right")

# Número de personas con padres separados por categoría
cantidad_personas <- c(sum(with(junior, Papas_separados == "SI")), 
                       sum(with(senior, Papas_separados == "SI")),
                       sum(with(mayor, Papas_separados == "SI")))

dist_personas <- data.frame(Categoria=c("Junior", "Señor", "Mayor"), Cantidad=cantidad_personas)
ggplot(dist_personas, aes(x=Categoria, y=Cantidad)) + geom_bar(stat = "identity",fill=rainbow(3)) + coord_flip() + labs(title="Número de personas con padres separados por categoría")+
  geom_text(aes(y=Cantidad, label=cantidad_personas), vjust=1.5, color="white", size=3.5, hjust="right")

```
# 2. Metodo de reducción de dimensión (LDA)


## 2.1 PCA base de datos PCA1

```{r, echo=TRUE,warning=FALSE,tidy=TRUE}
df2<-read.csv("Ejercicio 2.1.csv", sep = ",")
res.pca <- PCA(df2[,-1], graph = F)

eig.val <- get_eigenvalue(res.pca)
eig.val
fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 90))

var <- get_pca_var(res.pca)

corrplot(var$cos2, is.corr=FALSE)

fviz_pca_var(res.pca, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE # Avoid text overlapping
)

fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")
)

fviz_pca_ind(res.pca,
             geom.ind = "point", # show points only (nbut not "text")
             col.ind =df2$tipo, # color by groups
             palette = c("#00AFBB", "#E7B800", "#FC4E07"),
             addEllipses = TRUE, # Concentration ellipses
             legend.title = "Groups")
```

## 2.2 PCA base de datos PCA2

```{r, echo=TRUE,tidy=TRUE}
url<-"http://web.stanford.edu/~hastie/ElemStatLearn//datasets/zip.digits/train.2"
data<-read.csv(url)

data.pca = scale(data)
data.pca <- prcomp(data, center = F, scale=T)
prop_varianza <- data.pca$sdev^2 / sum(data.pca$sdev^2)
prop_varianza_acum <- cumsum(prop_varianza)

ggplot(data = data.frame(prop_varianza, pc = 1:256),
       aes(x = pc, y = prop_varianza)) +
  xlim(0, 50) +
  geom_col(width = 0.3) +
  scale_y_continuous(limits = c(0,.5)) +
  theme_bw() +
  labs(x = "Componente principal",
       y = "Prop. de varianza explicada")

ggplot(data = data.frame(prop_varianza_acum, pc = 1:256),
       aes(x = pc, y = prop_varianza_acum, group = 1)) +
  geom_point() +
  geom_line() +
  geom_label(aes(label = round(prop_varianza_acum,2))) +
  theme_bw() +
  labs(x = "Componente principal",
       y = "Prop. varianza explicada acumulada")
```

# 3. Metodo de reducción de dimensión (PCA)

## 3.1 Exploren los datos y encuentren gráficamente sus tendencias y sus posibles correlaciones en gráficos 2D y 3D.

```{r, echo=TRUE,tidy=TRUE}
datos<-read.csv("Ejercicio 3.csv", sep = ",")
p1 <- ggplot(data = datos, aes(x = largo, fill = tipo)) +
  geom_histogram(position = "identity", alpha = 0.5)
p2 <- ggplot(data = datos, aes(x = ancho, fill = tipo)) +
  geom_histogram(position = "identity", alpha = 0.5)
p3 <- ggplot(data = datos, aes(x = alto, fill = tipo)) +
  geom_histogram(position = "identity", alpha = 0.5)
ggarrange(p1, p2, p3, nrow = 3, common.legend = TRUE)
pairs(x = datos[, c("largo","ancho","alto")],
      col = c("blue", "green","red")[datos$tipo], pch = 19,main="Grafica Correlacion 2D")

scatterplot3d(datos$largo, datos$ancho, datos$alto,
              color = c("firebrick", "green","blue")[datos$tipo], pch = 19,
              grid = TRUE, xlab = "largo", ylab = "ancho",
              zlab = "alto", angle = 65, cex.axis = 0.6,main = "Visualizacion 3D")
legend("topleft",
       bty = "n", cex = 0.8,
       title = "tipo",
       c("rectangular", "cuadrada", "circular"), fill = c("firebrick", "green","blue"))
```
## 3.2 Realicen la verificación de supuestos.
```{r, echo=TRUE,tidy=TRUE}
datos_tidy <- melt(datos, value.name = "valor")
kable(datos_tidy %>% group_by(tipo, variable) %>% summarise(p_value_Shapiro.test = shapiro.test(valor)$p.value))
outliers <- mvn(data = datos[,-1], mvnTest = "hz", multivariateOutlierMethod = "quan")
royston_test <- mvn(data = datos[,-1], mvnTest = "royston", multivariatePlot = "qq")
royston_test$multivariateNormality
hz_test <- mvn(data = datos[,-1], mvnTest = "hz")
hz_test$multivariateNormality
boxM(data = datos[,2:4], grouping = datos[,1])
```
## 3.3 Apliquen el método LDA o QDA según consideren conveniente paso a paso.
```{r, echo=TRUE,tidy=TRUE}
modelo_lda <- lda(formula = tipo ~ largo + ancho + alto,
                  data = datos)
```
## 3.4 Grafiquen como quedaría la clasificación y calculen su error para nuevos datos.
```{r, echo=TRUE,tidy=TRUE}
nuevas_observaciones <- data.frame(largo = 30, ancho = 15,
                                   alto = 20)
predict(object = modelo_lda, newdata = nuevas_observaciones)
predicciones <- predict(object = modelo_lda, newdata = datos[, -1],
                        method = "predictive")
table(datos$tipo, predicciones$class,
      dnn = c("Clase real", "Clase predicha"))
trainig_error <- mean(datos$especie != predicciones$class) * 100
paste("trainig_error=", trainig_error, "%")
with(datos, {
  s3d <- scatterplot3d(largo, ancho, alto,
                       color = c("firebrick", "green","blue")[datos$tipo],
                       pch = 19, grid = TRUE, xlab = "largo", ylab = "ancho",
                       zlab = "altura", angle = 65, cex.axis = 0.6,main = "Clasificacion de los Nuevos Datos")
  
  s3d.coords <- s3d$xyz.convert(largo, ancho, alto)
  # convierte coordenadas 3D en proyecciones 2D
  
  legend("topleft", 
         bty = "n", cex = 0.8,
         title = "Tipo",
         c("largo", "ancho","alto"), fill = c("firebrick", "green","blue"))
})
```